{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_0K4yBlYJIIM"
   },
   "outputs": [],
   "source": [
    "# Check GPU on Google Colab\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5322,
     "status": "ok",
     "timestamp": 1553435956609,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "WJ1sBpzUJ2oV",
    "outputId": "d05fdc32-4cb9-43d9-8414-1678e54443da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.1.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dldlEmHDKd0P"
   },
   "outputs": [],
   "source": [
    "#import torch\n",
    "#torch.__version__\n",
    "#torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18859,
     "status": "ok",
     "timestamp": 1553435970193,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "Duo7zF-lKx4s",
    "outputId": "170327bb-c9ae-4278-dcad-519bd58930ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/6c93685bed0026b6a1cce55ab173f6b617f6db0d1325d25489c2fd43e711/gensim-3.7.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.2MB 1.6MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.115)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.115 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.115)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.115->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.115->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Installing collected packages: gensim\n",
      "  Found existing installation: gensim 3.6.0\n",
      "    Uninstalling gensim-3.6.0:\n",
      "      Successfully uninstalled gensim-3.6.0\n",
      "Successfully installed gensim-3.7.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1100,
     "status": "ok",
     "timestamp": 1553436026012,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "CE6nWOVVE2i-",
    "outputId": "0345e0d2-54af-4c70-da91-4f355c4af053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Yan\n",
      "[nltk_data]     Zheng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unSdGfunG1YW"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gyDbbFAHnESb"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6042,
     "status": "ok",
     "timestamp": 1553436030994,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "T-aKzjFXAE_c",
    "outputId": "fd267ebf-6bb9-4b2a-deec-432e0028a180"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x205e9371150>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0XoDB-J8Kdjd"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "# text = open('pickup_lines_all.txt', 'r', encoding = \"utf8\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = []\n",
    "with open('pickup_lines_cleaned.json', 'r', encoding = \"utf8\") as json_data:\n",
    "    for line in json_data: \n",
    "        data1.append(json.loads(line))\n",
    "\n",
    "data2 = []\n",
    "for line in range(len(data1)): \n",
    "    data2.append(data1[line][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated and empty data\n",
    "text = [i for i in list(set(data2)) if i != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6050,
     "status": "ok",
     "timestamp": 1553436031021,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "uHG0dKDvDXY1",
    "outputId": "b3896dc9-b5c6-47b0-f702-6e02d9e7f108"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Java number I can call you up at?',\n",
       " 'I tried to send you something sexy last night but the mail man told me to get out of the mailbox',\n",
       " \"I think I'm called to marriage, can I call you sometime?\",\n",
       " \"Is that a keg in your pants? Because I'd love to tap that ass.\",\n",
       " 'Your cave or mine?']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17528"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4wr2MOucN1A1"
   },
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "# tokenized_text = [[list(map(str.lower, word_tokenize(word))) for word in sent_tokenize(line)] for line in text]\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(line))) for line in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9607,
     "status": "ok",
     "timestamp": 1553436034612,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "dXnUlhuMTdQg",
    "outputId": "47bee321-e6d4-4805-a1d3-0199fc11e95d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(token) for token in tokenized_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for token in tokenized_text: \n",
    "    if len(token) > 1000: \n",
    "        count += 1\n",
    "print(count)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Uqr9ULDaz28"
   },
   "outputs": [],
   "source": [
    "class PickupLines(nn.Module):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        \n",
    "        # Initialize the vocab \n",
    "        special_tokens = {'<pad>': 0, '<unk>':1, '<s>':2, '</s>':3}\n",
    "        self.vocab = Dictionary(texts)\n",
    "        self.vocab.patch_with_special_tokens(special_tokens)\n",
    "        \n",
    "        # Keep track of the vocab size.\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Keep track of how many data points.\n",
    "        self._len = len(texts)\n",
    "        \n",
    "        # Find the longest text in the data.\n",
    "        self.max_len = max(len(txt) for txt in texts) \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        vectorized_sent = self.vectorize(self.texts[index])\n",
    "        x_len = len(vectorized_sent)\n",
    "        # To pad the sentence:\n",
    "        # Pad left = 0; Pad right = max_len - len of sent.\n",
    "        pad_dim = (0, self.max_len - len(vectorized_sent))\n",
    "        vectorized_sent = F.pad(vectorized_sent, pad_dim, 'constant')\n",
    "        return {'x':vectorized_sent[:-1], \n",
    "                'y':vectorized_sent[1:], \n",
    "                'x_len':x_len}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens, start_idx=2, end_idx=3):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        # Lets just cast list of indices into torch tensors directly =)\n",
    "        \n",
    "        vectorized_sent = [start_idx] + self.vocab.doc2idx(tokens) + [end_idx]\n",
    "        return torch.tensor(vectorized_sent)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f13lRuOKnHex"
   },
   "outputs": [],
   "source": [
    "pickuplines_data = PickupLines(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9891,
     "status": "ok",
     "timestamp": 1553436034958,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "dP0nevQY8C6a",
    "outputId": "4893ec52-2bb3-4919-cb3b-36c84c1c97be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17269"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pickuplines_data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10368,
     "status": "ok",
     "timestamp": 1553436035446,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "SjaoQbnX61yq",
    "outputId": "22dfb6ab-9885-4291-a640-a1312f82a377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[   2,   22, 5505,  ...,    0,    0,    0],\n",
      "        [   2,    4, 1321,  ...,    0,    0,    0],\n",
      "        [   2,    4, 6817,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,   44, 1359,  ...,    0,    0,    0],\n",
      "        [   2,   57,    8,  ...,    0,    0,    0],\n",
      "        [   2,  285,  106,  ...,    0,    0,    0]]), 'y': tensor([[   22,  5505,  9890,  ...,     0,     0,     0],\n",
      "        [    4,  1321,   147,  ...,     0,     0,     0],\n",
      "        [    4,  6817,    18,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   44,  1359,   841,  ...,     0,     0,     0],\n",
      "        [   57,     8,    73,  ...,     0,     0,     0],\n",
      "        [  285,   106, 15680,  ...,     0,     0,     0]]), 'x_len': tensor([28, 24, 20, 19, 19, 19, 18, 16, 16, 12])}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "dataloader = DataLoader(dataset=pickuplines_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for data_dict in dataloader:\n",
    "    # Sort indices of data in batch by lengths.\n",
    "    sorted_indices = np.array(data_dict['x_len']).argsort()[::-1].tolist()\n",
    "    data_batch = {name:_tensor[sorted_indices]\n",
    "                  for name, _tensor in data_dict.items()}\n",
    "    print(data_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XcWilYhO69oE"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initialize the embedding layer with the \n",
    "        # - size of input (i.e. no. of words in input vocab)\n",
    "        # - no. of hidden nodes in the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        \n",
    "        # Initialize the GRU with the \n",
    "        # - size of the input (i.e. embedding layer)\n",
    "        # - size of the hidden layer \n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Initialize the \"classifier\" layer to map the RNN outputs\n",
    "        # to the vocabulary. Remember we need to -1 because the \n",
    "        # vectorized sentence we left out one token for both x and y:\n",
    "        # - size of hidden_size of the GRU output.\n",
    "        # - size of vocabulary\n",
    "        self.classifier = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs, use_softmax=False, hidden=None):\n",
    "        # Look up for the embeddings for the input word indices.\n",
    "        embedded = self.embedding(inputs)\n",
    "        # Put the embedded inputs into the GRU.\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        # Matrix manipulation magic.\n",
    "        batch_size, sequence_len, hidden_size = output.shape\n",
    "        # Technically, linear layer takes a 2-D matrix as input, so more manipulation...\n",
    "        output = output.contiguous().view(batch_size * sequence_len, hidden_size)\n",
    "        # Apply dropout.\n",
    "        output = F.dropout(output, 0.5)\n",
    "        # Put it through the classifier\n",
    "        # And reshape it to [batch_size x sequence_len x vocab_size]\n",
    "        output = self.classifier(output).view(batch_size, sequence_len, -1)\n",
    "        \n",
    "        return (F.softmax(output,dim=2), hidden) if use_softmax else (output, hidden)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSMORGxZ9plc"
   },
   "outputs": [],
   "source": [
    "# Set the hidden_size of the GRU \n",
    "embed_size = 12\n",
    "hidden_size = 10\n",
    "num_layers = 1\n",
    "\n",
    "_encoder = Generator(len(pickuplines_data.vocab), embed_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utfBE2BF9yhq"
   },
   "outputs": [],
   "source": [
    "# Take a batch.\n",
    "batch_size = 15\n",
    "dataloader = DataLoader(dataset=pickuplines_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "_batch = next(iter(dataloader))\n",
    "_inputs, _lengths = _batch['x'], _batch['x_len']\n",
    "_targets = _batch['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15630,
     "status": "ok",
     "timestamp": 1553436040747,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "hkA1DXO-93cf",
    "outputId": "6b58245f-e08c-4833-dec1-5c53e03cf4dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output sizes:\t torch.Size([15, 170, 17269])\n",
      "Input sizes:\t 15 170 17269\n",
      "Target sizes:\t torch.Size([15, 170])\n"
     ]
    }
   ],
   "source": [
    "_output, _hidden = _encoder(_inputs)\n",
    "print('Output sizes:\\t', _output.shape)\n",
    "print('Input sizes:\\t', batch_size, pickuplines_data.max_len -1, len(pickuplines_data.vocab))\n",
    "print('Target sizes:\\t', _targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15620,
     "status": "ok",
     "timestamp": 1553436040748,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "3HF8pada9_uK",
    "outputId": "724c11f2-73cb-4783-851d-9fe9dc111443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 170, 17269])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15610,
     "status": "ok",
     "timestamp": 1553436040750,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "Fcv-2jEl-CCJ",
    "outputId": "20a4aa83-2579-4822-e548-c5a21e326b65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([170, 17269])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26685,
     "status": "ok",
     "timestamp": 1553436051842,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "7_QrRnwo-Faf",
    "outputId": "b063dfc6-7829-44ba-bd89-dca906330f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 17269])\n"
     ]
    }
   ],
   "source": [
    "_, predicted_indices = torch.max(_output, dim=1)\n",
    "print(predicted_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26676,
     "status": "ok",
     "timestamp": 1553436051845,
     "user": {
      "displayName": "Nyon Yan Zheng",
      "photoUrl": "",
      "userId": "02912262567516669188"
     },
     "user_tz": -480
    },
    "id": "xAJ3z7fq-IgC",
    "outputId": "22c84b1b-eb16-44f6-b86e-817f76d8808f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hyperparams(embed_size=250, hidden_size=250, num_layers=1, loss_func=<class 'torch.nn.modules.loss.CrossEntropyLoss'>, learning_rate=0.03, optimizer=<class 'torch.optim.adam.Adam'>, batch_size=245)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "_hyper = ['embed_size', 'hidden_size', 'num_layers',\n",
    "          'loss_func', 'learning_rate', 'optimizer', 'batch_size']\n",
    "Hyperparams = namedtuple('Hyperparams', _hyper)\n",
    "\n",
    "\n",
    "hyperparams = Hyperparams(embed_size=250, hidden_size=250, num_layers=1,\n",
    "                          loss_func=nn.CrossEntropyLoss,\n",
    "                          learning_rate=0.03, optimizer=optim.Adam, batch_size=245)\n",
    "\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qhz9HJkT-K3R"
   },
   "outputs": [],
   "source": [
    "# Training routine.\n",
    "def train(num_epochs, dataloader, model, criterion, optimizer):\n",
    "    losses = []\n",
    "    plt.ion()\n",
    "    for _e in range(num_epochs):\n",
    "        for batch in tqdm(dataloader):\n",
    "            # Zero gradient.\n",
    "            optimizer.zero_grad()\n",
    "            x = batch['x'].to(device)\n",
    "            x_len = batch['x_len'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            # Feed forward. \n",
    "            output, hidden = model(x, use_softmax=False)\n",
    "            # Compute loss:\n",
    "            # Shape of the `output` is [batch_size x sequence_len x vocab_size]\n",
    "            # Shape of `y` is [batch_size x sequence_len]\n",
    "            # CrossEntropyLoss expects `output` to be [batch_size x vocab_size x sequence_len]\n",
    "            _, prediction = torch.max(output, dim=2)\n",
    "            loss = criterion(output.permute(0, 2, 1), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.float().data)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(losses)\n",
    "        plt.pause(0.05)\n",
    "\n",
    "\n",
    "def initialize_data_model_optim_loss(hyperparams):\n",
    "    # Initialize the dataset and dataloader.\n",
    "    pickuplines_data = PickupLines(tokenized_text)\n",
    "    dataloader = DataLoader(dataset=pickuplines_data, \n",
    "                            batch_size=hyperparams.batch_size, \n",
    "                            shuffle=True)\n",
    "\n",
    "    # Loss function.\n",
    "    criterion = hyperparams.loss_func(ignore_index=pickuplines_data.vocab.token2id['<pad>'], \n",
    "                                      reduction='mean')\n",
    "\n",
    "    # Model.\n",
    "    model = Generator(len(pickuplines_data.vocab), hyperparams.embed_size, \n",
    "                      hyperparams.hidden_size, hyperparams.num_layers).to(device)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = hyperparams.optimizer(model.parameters(), lr=hyperparams.learning_rate)\n",
    "    \n",
    "    return dataloader, model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IsOzOyYo-O3I"
   },
   "outputs": [],
   "source": [
    "def generate_example(model, temperature=1.0, max_len=100, hidden_state=None):\n",
    "    start_token, start_idx = '<s>', 2\n",
    "    # Start state.\n",
    "    inputs = torch.tensor(pickuplines_data.vocab.token2id[start_token]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    sentence = [start_token]\n",
    "    i = 0\n",
    "    while i < max_len and sentence[-1] not in ['</s>', '<pad>']:\n",
    "        i += 1\n",
    "        \n",
    "        embedded = model.embedding(inputs)\n",
    "        output, hidden_state = model.gru(embedded, hidden_state)\n",
    "\n",
    "        batch_size, sequence_len, hidden_size = output.shape\n",
    "        output = output.contiguous().view(batch_size * sequence_len, hidden_size)    \n",
    "        output = model.classifier(output).view(batch_size, sequence_len, -1).squeeze(0)\n",
    "        #_, prediction = torch.max(F.softmax(output, dim=2), dim=2)\n",
    "        \n",
    "        word_weights = output.div(temperature).exp().cpu()\n",
    "        if len(word_weights.shape) > 1:\n",
    "            word_weights = word_weights[-1] # Pick the last word.    \n",
    "        word_idx = torch.multinomial(word_weights, 1).view(-1)\n",
    "        \n",
    "        sentence.append(kilgariff_data.vocab[int(word_idx)])\n",
    "        \n",
    "        inputs = tensor([pickuplines_data.vocab.token2id[word] for word in sentence]).unsqueeze(0).to(device)\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wfGhFJlN-SdH",
    "outputId": "e39d7ca2-6938-4119-e895-a39c58296821"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/65 [00:00<?, ?it/s]\n",
      "  2%|█▏                                                                              | 1/65 [05:26<5:47:56, 326.19s/it]\n",
      "  3%|██▍                                                                             | 2/65 [10:47<5:41:05, 324.85s/it]\n",
      "  5%|███▋                                                                            | 3/65 [16:09<5:34:30, 323.71s/it]\n",
      "  6%|████▉                                                                           | 4/65 [20:43<5:14:02, 308.89s/it]\n",
      "  8%|██████▏                                                                         | 5/65 [25:24<5:00:27, 300.46s/it]\n",
      "  9%|███████▍                                                                        | 6/65 [30:04<4:49:23, 294.30s/it]"
     ]
    }
   ],
   "source": [
    "hyperparams = Hyperparams(embed_size=250, hidden_size=250, num_layers=1,\n",
    "                          loss_func=nn.CrossEntropyLoss,\n",
    "                          learning_rate=0.03, optimizer=optim.Adam, batch_size=250)\n",
    "\n",
    "dataloader, model, optimizer, criterion = initialize_data_model_optim_loss(hyperparams)\n",
    "\n",
    "train(100, dataloader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IT5TbUGeAcqa"
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    generate_example(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text Processing CA v1",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
